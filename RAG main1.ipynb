{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "692948a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jimin/miniconda3/envs/ai2/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# 0. 라이브러리 임포트\n",
    "# =============================\n",
    "import os\n",
    "import torch\n",
    "from langchain_community.document_loaders import PyPDFLoader, TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.llms import HuggingFacePipeline\n",
    "from langchain.chains import RetrievalQA\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b194e7ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 1 문서 로딩 완료\n"
     ]
    }
   ],
   "source": [
    "# 1. 문서 로딩 (PDF 또는 TXT)\n",
    "# =============================\n",
    "file_path = \"RAG용 데이터셋.txt\"    # 👉 txt/pdf 지원\n",
    "ext = os.path.splitext(file_path)[1].lower()\n",
    "\n",
    "if ext == \".pdf\":\n",
    "    loader = PyPDFLoader(file_path)\n",
    "elif ext == \".txt\":\n",
    "    loader = TextLoader(file_path, encoding=\"utf-8\")\n",
    "else:\n",
    "    raise ValueError(f\"지원하지 않는 파일 형식: {ext}\")\n",
    "\n",
    "documents = loader.load()\n",
    "print(f\"총 {len(documents)} 문서 로딩 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97e79638",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 79 청크 생성 완료\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5195/4150633209.py:13: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 벡터스토어 저장 완료\n"
     ]
    }
   ],
   "source": [
    "# 2. 텍스트 분할 (더 작게 쪼개기)\n",
    "# =============================\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=150,   # ✅ 토큰 과다 방지\n",
    "    chunk_overlap=30\n",
    ")\n",
    "docs = text_splitter.split_documents(documents)\n",
    "print(f\"총 {len(docs)} 청크 생성 완료\")\n",
    "\n",
    "# =============================\n",
    "# 3. 임베딩 + 벡터DB\n",
    "# =============================\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\",\n",
    "    model_kwargs={\"device\": \"cpu\"}   # ✅ CPU에서 임베딩\n",
    ")\n",
    "\n",
    "vectorstore = FAISS.from_documents(docs, embeddings)\n",
    "vectorstore.save_local(\"gyeonggijeon_faiss_opt\")\n",
    "print(\"✅ 벡터스토어 저장 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61836dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:21<00:00,  5.35s/it]\n",
      "Device set to use cuda:0\n",
      "/tmp/ipykernel_5195/699763103.py:33: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFacePipeline``.\n",
      "  llm = HuggingFacePipeline(pipeline=rag_pipeline)\n"
     ]
    }
   ],
   "source": [
    "# 4. LLaMA3.1 모델 로드 (4bit)\n",
    "# =============================\n",
    "HF_TOKEN = \"\"  # 본인 HuggingFace 토큰 입력\n",
    "model_name = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, token=HF_TOKEN)\n",
    "\n",
    "bnb_config = {\n",
    "    \"load_in_4bit\": True,\n",
    "    \"bnb_4bit_quant_type\": \"nf4\",\n",
    "    \"bnb_4bit_use_double_quant\": True,\n",
    "    \"bnb_4bit_compute_dtype\": torch.bfloat16,\n",
    "}\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",          # 자동 분산 로딩(GPU/CPU)\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    token=HF_TOKEN,\n",
    "    **bnb_config\n",
    ")\n",
    "\n",
    "rag_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device_map=\"auto\",\n",
    "    max_new_tokens=64,   # ✅ 답변 짧게 제한\n",
    "    temperature=0.7,\n",
    "    top_p=0.9\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=rag_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8f18509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. RAG QA 체인 구성 (stuff 체인)\n",
    "# =============================\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 1})  # ✅ 문서 1개만 검색\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    chain_type=\"stuff\",     # ✅ map_reduce 대신 stuff → 속도 개선\n",
    "    return_source_documents=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "a3e7dcb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🙋 질문: '태실'에는 예종대왕의 유골이 묻혀있나요?\n",
      "📝 답변: Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "기전 정전과 조경묘만 남아 있다가 최근 들어 부속건물을 복원하여 지금의 조경묘가\n",
      "된 것이지요.\n",
      "전주가 조선왕조의 발원지임을 상징하는 조경묘~!! 잠시 관람 하신 후 어진 박\n",
      "물관으로 안내해드리겠습니다\n",
      "\n",
      "Question: '태실'에는 예종대왕의 유골이 묻혀있나요?\n",
      "Helpful Answer: 안뇨. (I don't know.) The text doesn't mention '태실' at all. It talks about '조경묘' which seems to be the name of a historical site, and mentions that it was recently restored, but it doesn't mention anything about '태실' or the\n"
     ]
    }
   ],
   "source": [
    "# 6. 질의응답 실행\n",
    "# =============================\n",
    "query = \"'태실'에는 예종대왕의 유골이 묻혀있나요?\"\n",
    "result = qa_chain.invoke({\"query\": query})\n",
    "\n",
    "print(\"🙋 질문:\", query)\n",
    "print(\"📝 답변:\", result[\"result\"].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691d6a94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
