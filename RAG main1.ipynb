{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "692948a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jimin/miniconda3/envs/ai2/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# 0. ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸\n",
    "# =============================\n",
    "import os\n",
    "import torch\n",
    "from langchain_community.document_loaders import PyPDFLoader, TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.llms import HuggingFacePipeline\n",
    "from langchain.chains import RetrievalQA\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b194e7ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì´ 1 ë¬¸ì„œ ë¡œë”© ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "# 1. ë¬¸ì„œ ë¡œë”© (PDF ë˜ëŠ” TXT)\n",
    "# =============================\n",
    "file_path = \"RAGìš© ë°ì´í„°ì…‹.txt\"    # ğŸ‘‰ txt/pdf ì§€ì›\n",
    "ext = os.path.splitext(file_path)[1].lower()\n",
    "\n",
    "if ext == \".pdf\":\n",
    "    loader = PyPDFLoader(file_path)\n",
    "elif ext == \".txt\":\n",
    "    loader = TextLoader(file_path, encoding=\"utf-8\")\n",
    "else:\n",
    "    raise ValueError(f\"ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹: {ext}\")\n",
    "\n",
    "documents = loader.load()\n",
    "print(f\"ì´ {len(documents)} ë¬¸ì„œ ë¡œë”© ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97e79638",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì´ 79 ì²­í¬ ìƒì„± ì™„ë£Œ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5195/4150633209.py:13: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ë²¡í„°ìŠ¤í† ì–´ ì €ì¥ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "# 2. í…ìŠ¤íŠ¸ ë¶„í•  (ë” ì‘ê²Œ ìª¼ê°œê¸°)\n",
    "# =============================\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=150,   # âœ… í† í° ê³¼ë‹¤ ë°©ì§€\n",
    "    chunk_overlap=30\n",
    ")\n",
    "docs = text_splitter.split_documents(documents)\n",
    "print(f\"ì´ {len(docs)} ì²­í¬ ìƒì„± ì™„ë£Œ\")\n",
    "\n",
    "# =============================\n",
    "# 3. ì„ë² ë”© + ë²¡í„°DB\n",
    "# =============================\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\",\n",
    "    model_kwargs={\"device\": \"cpu\"}   # âœ… CPUì—ì„œ ì„ë² ë”©\n",
    ")\n",
    "\n",
    "vectorstore = FAISS.from_documents(docs, embeddings)\n",
    "vectorstore.save_local(\"gyeonggijeon_faiss_opt\")\n",
    "print(\"âœ… ë²¡í„°ìŠ¤í† ì–´ ì €ì¥ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61836dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:21<00:00,  5.35s/it]\n",
      "Device set to use cuda:0\n",
      "/tmp/ipykernel_5195/699763103.py:33: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFacePipeline``.\n",
      "  llm = HuggingFacePipeline(pipeline=rag_pipeline)\n"
     ]
    }
   ],
   "source": [
    "# 4. LLaMA3.1 ëª¨ë¸ ë¡œë“œ (4bit)\n",
    "# =============================\n",
    "HF_TOKEN = \"\"  # ë³¸ì¸ HuggingFace í† í° ì…ë ¥\n",
    "model_name = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, token=HF_TOKEN)\n",
    "\n",
    "bnb_config = {\n",
    "    \"load_in_4bit\": True,\n",
    "    \"bnb_4bit_quant_type\": \"nf4\",\n",
    "    \"bnb_4bit_use_double_quant\": True,\n",
    "    \"bnb_4bit_compute_dtype\": torch.bfloat16,\n",
    "}\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",          # ìë™ ë¶„ì‚° ë¡œë”©(GPU/CPU)\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    token=HF_TOKEN,\n",
    "    **bnb_config\n",
    ")\n",
    "\n",
    "rag_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device_map=\"auto\",\n",
    "    max_new_tokens=64,   # âœ… ë‹µë³€ ì§§ê²Œ ì œí•œ\n",
    "    temperature=0.7,\n",
    "    top_p=0.9\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=rag_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8f18509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. RAG QA ì²´ì¸ êµ¬ì„± (stuff ì²´ì¸)\n",
    "# =============================\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 1})  # âœ… ë¬¸ì„œ 1ê°œë§Œ ê²€ìƒ‰\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    chain_type=\"stuff\",     # âœ… map_reduce ëŒ€ì‹  stuff â†’ ì†ë„ ê°œì„ \n",
    "    return_source_documents=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "a3e7dcb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ™‹ ì§ˆë¬¸: 'íƒœì‹¤'ì—ëŠ” ì˜ˆì¢…ëŒ€ì™•ì˜ ìœ ê³¨ì´ ë¬»í˜€ìˆë‚˜ìš”?\n",
      "ğŸ“ ë‹µë³€: Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "ê¸°ì „ ì •ì „ê³¼ ì¡°ê²½ë¬˜ë§Œ ë‚¨ì•„ ìˆë‹¤ê°€ ìµœê·¼ ë“¤ì–´ ë¶€ì†ê±´ë¬¼ì„ ë³µì›í•˜ì—¬ ì§€ê¸ˆì˜ ì¡°ê²½ë¬˜ê°€\n",
      "ëœ ê²ƒì´ì§€ìš”.\n",
      "ì „ì£¼ê°€ ì¡°ì„ ì™•ì¡°ì˜ ë°œì›ì§€ì„ì„ ìƒì§•í•˜ëŠ” ì¡°ê²½ë¬˜~!! ì ì‹œ ê´€ëŒ í•˜ì‹  í›„ ì–´ì§„ ë°•\n",
      "ë¬¼ê´€ìœ¼ë¡œ ì•ˆë‚´í•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤\n",
      "\n",
      "Question: 'íƒœì‹¤'ì—ëŠ” ì˜ˆì¢…ëŒ€ì™•ì˜ ìœ ê³¨ì´ ë¬»í˜€ìˆë‚˜ìš”?\n",
      "Helpful Answer: ì•ˆë‡¨. (I don't know.) The text doesn't mention 'íƒœì‹¤' at all. It talks about 'ì¡°ê²½ë¬˜' which seems to be the name of a historical site, and mentions that it was recently restored, but it doesn't mention anything about 'íƒœì‹¤' or the\n"
     ]
    }
   ],
   "source": [
    "# 6. ì§ˆì˜ì‘ë‹µ ì‹¤í–‰\n",
    "# =============================\n",
    "query = \"'íƒœì‹¤'ì—ëŠ” ì˜ˆì¢…ëŒ€ì™•ì˜ ìœ ê³¨ì´ ë¬»í˜€ìˆë‚˜ìš”?\"\n",
    "result = qa_chain.invoke({\"query\": query})\n",
    "\n",
    "print(\"ğŸ™‹ ì§ˆë¬¸:\", query)\n",
    "print(\"ğŸ“ ë‹µë³€:\", result[\"result\"].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691d6a94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
